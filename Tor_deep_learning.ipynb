{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Drime648/tor-deep-learning/blob/main/Tor_deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3tV6fkuh8dw"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import os, os.path\n",
        "import pathlib\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import OneHotEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "q0w6GwTQI7Fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L_0lOrHT9AT"
      },
      "source": [
        "# Extracting the data\n",
        "\n",
        "We extract the data, split it, and put it into train and test dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzv_iCXt6JYg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unVfAKmA6nQj"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/kdsec/wangknn-dataset/raw/master/data.zip\n",
        "import zipfile\n",
        "zipref = zipfile.ZipFile(\"data.zip\")\n",
        "zipref.extractall()\n",
        "# zipref.close()\n",
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
        "from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, walk_through_dir\n",
        "\n",
        "unzip_data(\"data.zip\")\n",
        "walk_through_dir(\"/content/data.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvyVQ5Mx-KHF"
      },
      "outputs": [],
      "source": [
        "# walk_through_dir(\"/content\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsdYlfI6-fic"
      },
      "outputs": [],
      "source": [
        "# !ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7Ne_xX3-rhL"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "\n",
        "train_df = pd.DataFrame(columns = [\"page_num\", \"trace_num\", \"trace_data\"])\n",
        "test_df = pd.DataFrame(columns = [\"page_num\", \"trace_num\", \"trace_data\"])\n",
        "# #72-18 split\n",
        "\n",
        "#page_num goes from 0-99, this is the classes we will predict\n",
        "#trace_num goes from 0-89, as each monitored site has 90 traces.\n",
        "#In total it is 90*100 = 9,000 traces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4MW5a-rC0vl"
      },
      "outputs": [],
      "source": [
        "import os, os.path\n",
        "import pathlib\n",
        "\n",
        "PATH = pathlib.Path(\"/content\")\n",
        "path, dirs, files = next(os.walk(PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N17tEuS2EiMj"
      },
      "outputs": [],
      "source": [
        "def get_lines(filename):\n",
        "  with open(filename) as f:\n",
        "    return f.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3GHDf9TE4QY"
      },
      "outputs": [],
      "source": [
        "def preprocess_lines(filename):\n",
        "  input_lines = get_lines(filename)\n",
        "  samples = []\n",
        "\n",
        "  for line in input_lines:\n",
        "    x = line.split()\n",
        "    samples.append(int(x[1]))\n",
        "\n",
        "    \n",
        "  return samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nF1dWIEyDBsE"
      },
      "outputs": [],
      "source": [
        "page_num = 0\n",
        "trace_num = 0\n",
        "trace_data = []\n",
        "for file in files:\n",
        "  if(\"-\" in file):\n",
        "    # print(file)\n",
        "    x = file.split(\"-\")\n",
        "    page_num = int(x[0])\n",
        "    trace_num = int(x[1])\n",
        "    trace_data = preprocess_lines(file)\n",
        "    if(len(trace_data) > 0):#Filter can be anything. Right now it is zero\n",
        "    \n",
        "      #make sure the length is exactly 5000\n",
        "      while(len(trace_data) < 5000):\n",
        "          trace_data.append(0)\n",
        "      \n",
        "      trace_data = trace_data[:5000]\n",
        "      # print(len(trace_data))\n",
        "      # append everything to the dataframe\n",
        "      temp_df = {\"page_num\": page_num, \"trace_num\": trace_num, \"trace_data\": trace_data}\n",
        "\n",
        "      #first, decide if it is test or train\n",
        "      #72-18 split\n",
        "      #trace nums go from 0-89\n",
        "      #0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17 - trace nums for the test dataset\n",
        "      if(trace_num >= 17): # add to the train dataset\n",
        "        train_df = train_df.append(temp_df, ignore_index=True)\n",
        "      else:\n",
        "        test_df = test_df.append(temp_df, ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwaUXFVZcNtN"
      },
      "source": [
        "Making a graph to see the ranges of the lengths of the trace data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1AWh_sWcHUc"
      },
      "outputs": [],
      "source": [
        "# page_num = 0\n",
        "# trace_num = 0\n",
        "# trace_data = []\n",
        "# for file in files:\n",
        "#   if(\"-\" in file):\n",
        "#     # print(file)\n",
        "#     x = file.split(\"-\")\n",
        "#     page_num = int(x[0])\n",
        "#     trace_num = int(x[1])\n",
        "#     trace_data = preprocess_lines(file)\n",
        "#     if(len(trace_data) > 0):#in case it is an outlier\n",
        "    \n",
        "#       #make sure the length is exactly 5000\n",
        "#       # while(len(trace_data) < 5000):\n",
        "#       #     trace_data.append(0)\n",
        "      \n",
        "#       # trace_data = trace_data[:5000]\n",
        "#       # print(len(trace_data))\n",
        "#       # append everything to the dataframe\n",
        "#       temp_df = {\"page_num\": page_num, \"trace_num\": trace_num, \"trace_data\": trace_data}\n",
        "\n",
        "#       #first, decide if it is test or train\n",
        "#       #72-18 split\n",
        "#       #trace nums go from 0-89\n",
        "#       #0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17 - trace nums for the test dataset\n",
        "#       train_df = train_df.append(temp_df, ignore_index=True)\n",
        "#       # if(trace_num >= 17): # add to the train dataset\n",
        "#       #   train_df = train_df.append(temp_df, ignore_index=True)\n",
        "#       # else:\n",
        "#       #   test_df = test_df.append(temp_df, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpYX1VaTcTr1"
      },
      "outputs": [],
      "source": [
        "# train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3GRLLU4cY7n"
      },
      "outputs": [],
      "source": [
        "# count = 0\n",
        "# num_array = []\n",
        "# for index, row in train_df.iterrows():\n",
        "#   num = len(row[\"trace_data\"])\n",
        "#   num_array.append(num)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRawa0YuevvA"
      },
      "outputs": [],
      "source": [
        "# from matplotlib import pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# plt.hist(num_array, bins = np.linspace(0, 11000, 20))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.figure(figsize=(9, 9))\n",
        "# plt.title('Trace sizes', size = 20)\n",
        "# plt.ylabel('# of traces', size = 20)\n",
        "# plt.xlabel('trace length', size = 20)\n",
        "# plt.hist(num_array, bins = np.linspace(0, 11000, 10))\n",
        "# plt.tick_params(axis='both', which='major', labelsize=20, width=2.5, length=10)\n",
        "# plt.savefig('Trace Sizes', bbox_inches='tight')\n",
        "# plt.show()\n",
        "# from google.colab import files\n",
        "# files.download('Trace Sizes.png')"
      ],
      "metadata": {
        "id": "qmEP0-7-PaOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OFO2LoCFVd0"
      },
      "outputs": [],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apAxPnYNKZA0"
      },
      "outputs": [],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYMHZPrhLNU1"
      },
      "outputs": [],
      "source": [
        "# train_df.to_csv(\"tor_dl_training_nofilter.csv\")\n",
        "# test_df.to_csv(\"tor_dl_testing_nofilter.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-YBv7xILvj5"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# files.download(\"tor_dl_training_nofilter.csv\") \n",
        "# files.download(\"tor_dl_testing_nofilter.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LD3lVd0zL3oX"
      },
      "outputs": [],
      "source": [
        "# !ls | grep \"tor\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1muYoRE_hbZ3"
      },
      "source": [
        "#Import the cleaned data now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tl53b6N7hbIR"
      },
      "outputs": [],
      "source": [
        "# train_df = pd.read_csv(\"https://raw.githubusercontent.com/Drime648/tor-deep-learning/main/tor_dl_training.csv\")\n",
        "# test_df = pd.read_csv(\"https://raw.githubusercontent.com/Drime648/tor-deep-learning/main/tor_dl_testing.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfSkVFDMiNjI"
      },
      "outputs": [],
      "source": [
        "#remove the index column\n",
        "# train_df = train_df.drop(\"Unnamed: 0\", axis = 1)\n",
        "# test_df = test_df.drop(\"Unnamed: 0\", axis = 1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmgF_nNvjJSs"
      },
      "outputs": [],
      "source": [
        "#randomize\n",
        "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
        "test_df = test_df.sample(frac=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiWE5oU_iRhZ"
      },
      "outputs": [],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91iVP09bimj2"
      },
      "outputs": [],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AY3r5XAGivo2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzqShcTKjQzk"
      },
      "source": [
        "# replicate original model from Paper\n",
        "\n",
        "Paper: https://core.ac.uk/download/pdf/229876143.pdf\n",
        "\n",
        "Fingerprinting Attack on Tor Anonymity using Deep Learning\n",
        "\n",
        "\n",
        "by Kota Abe and Shigeki Goto\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwyTcee0lORI"
      },
      "source": [
        "#One hot encode data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRWewydolRoS"
      },
      "outputs": [],
      "source": [
        "one_hot_encoder = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFx1OMudDG-q"
      },
      "outputs": [],
      "source": [
        "# add one more to test data\n",
        "# temp_df = {\"page_num\": 300, \"trace_num\": 0, \"trace_data\": 0}\n",
        "# test_df = test_df.append(temp_df, ignore_index = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0LH--24RCgt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QRR-ed0lSGA"
      },
      "outputs": [],
      "source": [
        "one_hot_test_labels = one_hot_encoder.fit(test_df[\"page_num\"].to_numpy().reshape(-1,1))\n",
        "# test_df = test_df[:-1]\n",
        "\n",
        "one_hot_test_labels = one_hot_encoder.transform(test_df[\"page_num\"].to_numpy().reshape(-1,1))\n",
        "one_hot_train_labels = one_hot_encoder.transform(train_df[\"page_num\"].to_numpy().reshape(-1,1))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asjw9LNlPUQL"
      },
      "outputs": [],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aw_fMejyPyc5"
      },
      "outputs": [],
      "source": [
        "one_hot_test_labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-ppYMK-rNYR"
      },
      "outputs": [],
      "source": [
        "one_hot_train_labels.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RPI8H6anej7"
      },
      "source": [
        "#process data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-rdysvflSe6"
      },
      "outputs": [],
      "source": [
        "train_data = []\n",
        "\n",
        "for index, row in train_df.iterrows():\n",
        "  # strings = row[\"trace_data\"][1:-1].split(\", \")\n",
        "  # # strings[0]\n",
        "  # # print(strings)\n",
        "  # int_array = [int(number) for number in strings]\n",
        "    train_data.append(row[\"trace_data\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WB9adfSpLQz"
      },
      "outputs": [],
      "source": [
        "test_data = []\n",
        "\n",
        "for index, row in test_df.iterrows():\n",
        "\n",
        "  # strings = row[\"trace_data\"][1:-1].split(\", \")\n",
        "  # # strings[0]\n",
        "  # # print(strings)\n",
        "  # int_array = [int(number) for number in strings]\n",
        "  test_data.append(row[\"trace_data\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "774cI1tslTpT"
      },
      "outputs": [],
      "source": [
        "train_data = np.array(train_data)\n",
        "test_data = np.array(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YH0BF2rLlTyl"
      },
      "outputs": [],
      "source": [
        "np.shape(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcrWAgnqqau3"
      },
      "outputs": [],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3JS34uWqknA"
      },
      "source": [
        "#Make the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Sr4wA-2qpg6"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aL5Sck5mn-96"
      },
      "outputs": [],
      "source": [
        "train_data = tf.cast(train_data, tf.float32)\n",
        "test_data = tf.cast(test_data, tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "te0ufyzVql-0"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, one_hot_train_labels))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, one_hot_test_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfeHte90qmJS"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
        "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYM4jxMcqmTM"
      },
      "outputs": [],
      "source": [
        "test_dataset, train_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoK3hM61qOFx"
      },
      "source": [
        "##SDAE + MLP\n",
        "\n",
        "Make 2 layers of SDAE, then followed by MLP\n",
        "\n",
        "5000 - 500 - 125 - 500 - 5000 - 100\n",
        "\n",
        "SDAE is: \n",
        "5000 - 500 - 125 - 500 - 5000\n",
        "\n",
        "lr = 0.001, batch size = 50\n",
        "\n",
        "\n",
        "MLP is:\n",
        "\n",
        "5000 -> 100\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BymxYK0pjh93"
      },
      "outputs": [],
      "source": [
        "# layer_1 = 5000\n",
        "# layer_2 = 750\n",
        "# layer_3 = 500\n",
        "# center_layer = 250\n",
        "\n",
        "# model_1 = tf.keras.Sequential([\n",
        "#   tf.keras.layers.Dense(layer_1, \"relu\", kernel_initializer=\"he_normal\", name=\"layer_1\"),\n",
        "#   tf.keras.layers.Dense(layer_2, \"relu\", kernel_initializer=\"he_normal\", name=\"layer_2\"),\n",
        "#   tf.keras.layers.Dense(center_layer, \"relu\", kernel_initializer=\"he_normal\", name=\"hidden_layer\"),\n",
        "#   tf.keras.layers.Dense(layer_2, \"relu\", kernel_initializer=\"he_normal\", name=\"layer_2_mirror\"),\n",
        "#   tf.keras.layers.Dense(layer_1, \"relu\", kernel_initializer=\"he_normal\", name=\"layer_1_mirror\"),\n",
        "#   tf.keras.layers.Dense(100, \"softmax\", kernel_initializer=\"he_normal\", name=\"output_layer\"),\n",
        "# ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhBjyvgPLpy1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def make_model_checkpoint(name, path = \"/content/model_exp/\"):\n",
        "  return tf.keras.callbacks.ModelCheckpoint(os.path.join(path, name), save_best_only = True, verbose = 0, monitor=\"val_accuracy\")\n",
        "  #, monitor=\"val_accuracy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68uxvOxyIVL6"
      },
      "outputs": [],
      "source": [
        "# model_1.compile(loss = \"categorical_crossentropy\",\n",
        "#                 optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "#                 metrics = [\"accuracy\"],\n",
        "#               )\n",
        "\n",
        "# history_1 = model_1.fit(train_dataset, epochs = 100,\n",
        "#                         validation_data=test_dataset, \n",
        "#                     callbacks = [make_model_checkpoint(\"model_1\")])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.keras.utils.plot_model(model_1, show_shapes=True,to_file=\"SDAE_model.png\",)\n",
        "# from google.colab import files\n",
        "# files.download(\"SDAE_model.png\")\n",
        "# #to_file=\"SDAE_model.png\","
      ],
      "metadata": {
        "id": "7Itp2khY-u1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eq_ZE7UAJP0H"
      },
      "outputs": [],
      "source": [
        "# new_model_1 = tf.keras.models.load_model(\"/content/model_exp/model_1\")\n",
        "# new_model_1.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGNwOxv2crYC"
      },
      "outputs": [],
      "source": [
        "# plt.figure(figsize=(9, 9))\n",
        "# plt.plot(history_1.history['val_accuracy'])\n",
        "# plt.title('SDAE testing accuracy', size=20)\n",
        "# plt.ylabel('accuracy', size=20)\n",
        "# plt.xlabel('epoch #', size=20)\n",
        "# plt.legend(['testing accuracy'], loc='upper left', fontsize=15)\n",
        "# plt.tick_params(axis='both', which='major', labelsize=20, width=2.5, length=10)\n",
        "\n",
        "# plt.savefig('2 layer SDAE testing accuracy.png', bbox_inches='tight')\n",
        "# plt.show()\n",
        "\n",
        "# files.download('2 layer SDAE testing accuracy.png')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /etc/*release "
      ],
      "metadata": {
        "id": "I0qHj26sUCEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cat /proc/cpuinfo"
      ],
      "metadata": {
        "id": "4jvaDBvWU4Cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aCfWhLlNF-n"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0e0FOx8Q9jf"
      },
      "source": [
        "##try triple layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idrMUKijQ9Ru"
      },
      "outputs": [],
      "source": [
        "# layer_1 = 5000\n",
        "# layer_2 = 750\n",
        "# layer_3 = 500\n",
        "# center_layer = 250\n",
        "\n",
        "# model_2 = tf.keras.Sequential([\n",
        "#   tf.keras.layers.Dense(layer_1, \"relu\"),\n",
        "#   tf.keras.layers.Dense(layer_2, \"relu\"),\n",
        "#   tf.keras.layers.Dense(layer_3, \"relu\"),\n",
        "#   tf.keras.layers.Dense(center_layer, \"relu\"),\n",
        "#   tf.keras.layers.Dense(layer_3, \"relu\"),\n",
        "#   tf.keras.layers.Dense(layer_2, \"relu\"),\n",
        "#   tf.keras.layers.Dense(layer_1, \"relu\"),\n",
        "#   tf.keras.layers.Dense(100, \"softmax\"),\n",
        "# ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DbgkXSYRARX"
      },
      "outputs": [],
      "source": [
        "# model_2.compile(loss = \"categorical_crossentropy\",\n",
        "#                 optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "#                 metrics = [\"accuracy\"],\n",
        "#               )\n",
        "\n",
        "# history_2 = model_2.fit(train_dataset, epochs = 100,\n",
        "#                         validation_data=test_dataset, \n",
        "#                     callbacks = [make_model_checkpoint(\"model_2\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEIzmHVPRBTo"
      },
      "outputs": [],
      "source": [
        "# new_model_2 = tf.keras.models.load_model(\"/content/model_exp/model_2\")\n",
        "# new_model_2.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPRCrnyuRTQI"
      },
      "outputs": [],
      "source": [
        "# plt.figure(figsize=(9, 9))\n",
        "# plt.plot(history_2.history['val_accuracy'])\n",
        "# plt.title('3 layer SDAE close world testing accuracy')\n",
        "# plt.ylabel('accuracy')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['testing accuracy'], loc='upper left')\n",
        "\n",
        "\n",
        "# plt.savefig('3 layer SDAE close world testing accuracy.png', bbox_inches='tight')\n",
        "# plt.show()\n",
        "\n",
        "# files.download('3 layer SDAE close world testing accuracy.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En71Me-W3yUr"
      },
      "source": [
        "#Improve model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd04x_2R8YHE"
      },
      "source": [
        "##model 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJoVQIIEnUt6"
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras.layers as layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPdgGEIf30EC"
      },
      "outputs": [],
      "source": [
        "# model_3 = tf.keras.Sequential([\n",
        "#   layers.Lambda(lambda x: tf.expand_dims(x, axis = 1)),\n",
        "#   layers.Conv1D(5000, 5, padding = 'causal', activation=\"relu\"),\n",
        "#   layers.GlobalMaxPooling1D(),\n",
        "#   layers.Dense(100, activation=\"softmax\")\n",
        "# ], name = \"Conv1D\")\n",
        "\n",
        "# model_3.compile(loss = \"categorical_crossentropy\",\n",
        "#                 optimizer = \"Adam\",\n",
        "#                 metrics = [\"accuracy\"])\n",
        "\n",
        "# history_3 = model_3.fit(train_dataset, epochs = 40,\n",
        "#                         validation_data=test_dataset, \n",
        "#                     callbacks = [make_model_checkpoint(\"model_3\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MURT76IOrOlf"
      },
      "outputs": [],
      "source": [
        "# new_model_3 = tf.keras.models.load_model(\"/content/model_exp/model_3\")\n",
        "# new_model_3.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfE87A2arUBT"
      },
      "outputs": [],
      "source": [
        "# plt.figure(figsize=(9, 9))\n",
        "# plt.plot(history_3.history['val_accuracy'])\n",
        "# plt.title('Conv1D close world testing accuracy')\n",
        "# plt.ylabel('accuracy')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['testing accuracy'], loc='upper left')\n",
        "\n",
        "\n",
        "# plt.savefig('Conv1D close world testing accuracy.png', bbox_inches='tight')\n",
        "# plt.show()\n",
        "\n",
        "# files.download('Conv1D close world testing accuracy.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dP5TEnPu0bS"
      },
      "source": [
        "model 3 got 88.29%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FF9ov1ruwPE"
      },
      "source": [
        "##model 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Clo02N3krg6-"
      },
      "outputs": [],
      "source": [
        "# model_4 = tf.keras.Sequential([\n",
        "#   layers.Dense(5000, activation=\"relu\"),\n",
        "#   layers.Dense(750, activation=\"relu\"),\n",
        "\n",
        "#   layers.Lambda(lambda x: tf.expand_dims(x, axis = 1)),\n",
        "#   layers.Conv1D(500, 5, padding = 'causal', activation=\"relu\"),\n",
        "#   layers.GlobalMaxPooling1D(),\n",
        "\n",
        "#   layers.Dense(750, activation=\"relu\"),\n",
        "#   layers.Dense(5000, activation=\"relu\"),\n",
        "\n",
        "\n",
        "#   layers.Dense(100, activation=\"softmax\")\n",
        "# ], name = \"Conv1D\")\n",
        "\n",
        "# model_4.compile(loss = \"categorical_crossentropy\",\n",
        "#                 optimizer = \"Adam\",\n",
        "#                 metrics = [\"accuracy\"])\n",
        "\n",
        "# history_4 = model_4.fit(train_dataset, epochs = 40,\n",
        "#                         validation_data=test_dataset, \n",
        "#                     callbacks = [make_model_checkpoint(\"model_4\")])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vd9JbqfyNcM"
      },
      "source": [
        "##model 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjKKzV_iyOjm"
      },
      "outputs": [],
      "source": [
        "# model_5 = tf.keras.models.Sequential([\n",
        "#   layers.Lambda(lambda x: tf.expand_dims(x, axis = 1)),\n",
        "#   layers.LSTM(5000, return_sequences=True, name=\"lstm_layer\"),\n",
        "#   layers.GlobalMaxPooling1D(),  \n",
        "#   layers.Dense(100, activation=\"softmax\")\n",
        "# ], name = \"LSTM-BASIC\")\n",
        "\n",
        "# model_5.compile(loss = \"categorical_crossentropy\",\n",
        "#                 optimizer = \"Adam\",\n",
        "#                 metrics = [\"accuracy\"])\n",
        "\n",
        "# history_5 = model_5.fit(train_dataset, epochs = 40,\n",
        "#                         validation_data=test_dataset, \n",
        "#                     callbacks = [make_model_checkpoint(\"model_5\")])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xln7psiNySNG"
      },
      "outputs": [],
      "source": [
        "# new_model_5 = tf.keras.models.load_model(\"/content/model_exp/model_5\")\n",
        "# new_model_5.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNYzklbS79Mg"
      },
      "source": [
        "88.11% accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lQbXpVkx8Ov"
      },
      "source": [
        "#Ensemble model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3yO1SneyVgW"
      },
      "outputs": [],
      "source": [
        "def make_2layer_SDAE(layer_1, center_layer, name):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(5000, \"relu\", kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(layer_1, \"relu\", kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(center_layer, \"relu\", kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(layer_1, \"relu\", kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(5000, \"relu\", kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(100, \"softmax\", kernel_initializer=\"he_normal\"),\n",
        "  ])\n",
        "  model.compile(loss = \"categorical_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                metrics = [\"accuracy\"],\n",
        "              )\n",
        "  history = model.fit(train_dataset, epochs = 100,\n",
        "                        validation_data=test_dataset, \n",
        "                    callbacks = [make_model_checkpoint(name)], verbose=None)\n",
        "  return (history, tf.keras.models.load_model(\"/content/model_exp/\"+name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFKoA7axy7J2"
      },
      "outputs": [],
      "source": [
        "def make_3layer_SDAE(layer_1, layer_2, center_layer):\n",
        "  model_1 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(5000, \"relu\", kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(layer_1, \"relu\", kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(layer_2, \"relu\", kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(center_layer, \"relu\", kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(layer_2, \"relu\", kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(layer_1, \"relu\", kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(5000, \"relu\", kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(100, \"softmax\", kernel_initializer=\"he_normal\"),\n",
        "  ])\n",
        "  model_1.compile(loss = \"categorical_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                metrics = [\"accuracy\"],\n",
        "              )\n",
        "  model_1.fit(train_dataset, epochs = 20,\n",
        "                        validation_data=test_dataset, \n",
        "                    callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True)], verbose=None)\n",
        "  \n",
        "  return model_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnPOdJgZzLJK"
      },
      "outputs": [],
      "source": [
        "ensemble_models = []\n",
        "histories = []\n",
        "#add in 2 layers\n",
        "\n",
        "i = 1000\n",
        "while i > 0:\n",
        "  j = i\n",
        "\n",
        "  while j > 0:\n",
        "    print(\"layer 1:\\t\" + str(i) + \"\\tlayer 2:\\t\" + str(j))\n",
        "    model = make_2layer_SDAE(i, j, \"model_\"+str(i)+\"_\"+str(j))\n",
        "    histories.append(model[0])\n",
        "    ensemble_models.append(model[1])\n",
        "    j -= 250\n",
        "  print(\"layer 1:\\t\" + str(i) + \"\\tlayer 2:\\t125\")\n",
        "  model = make_2layer_SDAE(i, 125, \"model_\"+str(i)+\"_\"+str(125))\n",
        "  histories.append(model[0])\n",
        "  ensemble_models.append(model[1])\n",
        "  i-=250\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbJHukk8g1KG"
      },
      "outputs": [],
      "source": [
        "ensemble_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRb7N3qK15DG"
      },
      "outputs": [],
      "source": [
        "def make_ensemble_preds(ensemble, input):\n",
        "  preds = []\n",
        "  for model in ensemble:\n",
        "    preds.append(one_hot_encoder.inverse_transform(model.predict(input)))\n",
        "    \n",
        "  return tf.constant(preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQXRcajfhl29"
      },
      "outputs": [],
      "source": [
        "x = make_ensemble_preds(ensemble_models, test_dataset)\n",
        "predictions = []\n",
        "for j in range(len(one_hot_test_labels)):\n",
        "  ans = []\n",
        "  for i in range(len(ensemble_models)):\n",
        "    ans.append(x[i][j])\n",
        "  a = tf.math.bincount(ans)\n",
        "  a = tf.math.argmax(a).numpy()\n",
        "  predictions.append(a)\n",
        "  # print(a)\n",
        "\n",
        "y_preds = np.array(predictions, dtype=int)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = test_df[\"page_num\"].to_numpy().astype(int)"
      ],
      "metadata": {
        "id": "fk7MaB7szWPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
      ],
      "metadata": {
        "id": "DVtMv6JIzbEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_true, y_preds)\n",
        "accuracy"
      ],
      "metadata": {
        "id": "4kwZ_ga2zecb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(y_true, y_pred):\n",
        "  #model accuracy\n",
        "  accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "  precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average = \"weighted\")\n",
        "\n",
        "  results = {\n",
        "      \"accuracy\": accuracy * 100,\n",
        "      \"precision\": precision * 100,\n",
        "      \"recall\": recall *100,\n",
        "      \"f1 score\": f1 * 100\n",
        "  }\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "jDTkW1DJ6_Zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_evaluation = evaluate(y_true, y_preds)\n",
        "ensemble_evaluation"
      ],
      "metadata": {
        "id": "uy2DrQyVJdSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds"
      ],
      "metadata": {
        "id": "DHwK_nA3NSvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "e2kUK39TmJUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(5000, \"relu\", kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(750, \"relu\", kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(500, \"relu\", kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(750, \"relu\", kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(5000, \"relu\", kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(100, \"softmax\", kernel_initializer=\"he_normal\"),\n",
        "])\n",
        "model_1.compile(loss = \"categorical_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                metrics = [\"accuracy\"],\n",
        "              )\n",
        "model_1.fit(train_dataset, epochs = 40,\n",
        "                        validation_data=test_dataset, \n",
        "                    callbacks = [make_model_checkpoint(\"model_1\")])\n",
        "\n",
        "new_model_1 = tf.keras.models.load_model(\"/content/model_exp/model_1\")\n",
        "new_model_1.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "jLnok7yV1ZlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer2_preds = one_hot_encoder.inverse_transform(new_model_1.predict(test_dataset))\n",
        "y_layer2_preds = np.array(layer2_preds, dtype=int)\n",
        "y_layer2_preds = y_layer2_preds.reshape(1, -1).flatten()\n",
        "sdae_evaluation = evaluate(y_true, y_layer2_preds)\n",
        "sdae_evaluation"
      ],
      "metadata": {
        "id": "xXQlsylT5XQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WyLwCX2-N0Ly"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPUwmJCGxFrJdlnQxnEAMGL",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}